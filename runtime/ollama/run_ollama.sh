#!/usr/bin/env bash
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# run_ollama.sh
#
#   â€¢ Starts Ollama on a single RTXâ€‘4090 (24â€¯GB VRAM).
#   â€¢ Uses flashâ€‘attention, sets a generous keepâ€‘alive, and limits
#     the number of loaded models to avoid VRAM thrashing.
#   â€¢ Exposes the server on all interfaces (0.0.0.0) â€“ change if you
#     want it bound only to localhost.
#   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  (c) 2025 by <yourâ€‘name> â€“ MITâ€‘style license (see README if you plan
#  to distribute this script).  Happy LLM-ing! ğŸš€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

set -euo pipefail   # safest Bash defaults

# ------------------------------------------------------------
#  1ï¸âƒ£  Environment variables â€“ tweak as needed
# ------------------------------------------------------------
export OLLAMA_DEBUG=0                             # 1 for verbose debugging, 0 for normal
export OLLAMA_HOST=0.0.0.0:11434                  # bind to all interfaces
export OLLAMA_KEEP_ALIVE=60m                      # keep a model loaded for 10â€¯min after idle
export OLLAMA_MAX_LOADED_MODELS=1                 # 1â€“2 models is safe on 24â€¯GB
export OLLAMA_MAX_QUEUE=32                        # max queued requests (higher = more backâ€‘pressure)
export OLLAMA_NUM_PARALLEL=1                      # parallel requests that can run simultaneously
export OLLAMA_ORIGINS="*"                         # allow all CORS origins (replace if you want tighter control)
export OLLAMA_FLASH_ATTENTION=1                   # enable flashâ€‘attention (requires CUDA 12+)
export OLLAMA_KV_CACHE_TYPE="f16"                 # memoryâ€‘efficient K/V cache
export OLLAMA_GPU_OVERHEAD=$((2*1024*1024*1024))  # reserve 2â€¯GB for overhead (bytes)
export OLLAMA_MODELS="/home/kimyj/.ollama/models" # where Ollama stores its models
export OLLAMA_SCHED_SPREAD=0                      # not needed for a single GPU
export OLLAMA_LLM_LIBRARY="llama.cpp"             # you can also use "vllm" if you have it

# ------------------------------------------------------------
#  2ï¸âƒ£  Make sure the model directory exists & has the right perms
# ------------------------------------------------------------
mkdir -p "$OLLAMA_MODELS"
chmod 700 "$OLLAMA_MODELS"

# ------------------------------------------------------------
#  3ï¸âƒ£  Optionally preâ€‘load a model (e.g. 7B or 13B) â€“ uncomment if you want a warm start
# ------------------------------------------------------------
# echo "Preâ€‘loading model 'gpt-oss:20b'..."
# ollama pull gpt-oss:20b
# echo "âœ…  Model loaded"

# ------------------------------------------------------------
#  4ï¸âƒ£  Launch Ollama
# ------------------------------------------------------------
echo "ğŸŸ¢ Starting Ollama server on ${OLLAMA_HOST}"
echo "ğŸŸ¢ Keepâ€‘alive: ${OLLAMA_KEEP_ALIVE}"
echo "ğŸŸ¢ Max loaded models: ${OLLAMA_MAX_LOADED_MODELS}"
echo "ğŸŸ¢ Flash attention: ${OLLAMA_FLASH_ATTENTION}"
echo "ğŸŸ¢ GPU overhead: ${OLLAMA_GPU_OVERHEAD} bytes"

# Run the server in the foreground; you can add '&' to background it
mkdir -p ~/.ollama/logs
ollama serve > ~/.ollama/logs/ollama.log 2>&1 &

# ------------------------------------------------------------
#  5ï¸âƒ£  Graceful shutdown handling (Ctrlâ€‘C)
# ------------------------------------------------------------
# When the script exits (via SIGINT or other), Ollama will terminate automatically.
# No explicit cleanup needed â€“ the server will free VRAM on exit.
